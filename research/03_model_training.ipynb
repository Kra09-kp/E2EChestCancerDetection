{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd148ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/kirti/Dev/DeepLearning/Project/E2E/ChestCancerDetection/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9831c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/kirti/Dev/DeepLearning/Project/E2E/ChestCancerDetection'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56793a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data : Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_learning_rate: float\n",
    "    params_image_size: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7945ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "from cnnClassifier.constants import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import tv_tensors\n",
    "from torchsummary import summary\n",
    "from torchvision.models import VGG16_Weights\n",
    "from torch.utils import data\n",
    "from torchvision.transforms import v2 as T\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca95e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "\n",
    "    def __init__(self, config_path: Path = CONFIG_FILE_PATH, \n",
    "                 params_path: Path = PARAMS_FILE_PATH):\n",
    "        \"\"\"        Initializes the ConfigManager with paths to the configuration and parameters files.\n",
    "        Args:\n",
    "            config_path (Path): Path to the configuration file.\n",
    "            params_path (Path): Path to the parameters file.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.train_model\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir,\"Data\")\n",
    "        create_directories(\n",
    "            [Path(training.root_dir)])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.model_save_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.update_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_learning_rate=params.LEARNING_RATE,\n",
    "            params_image_size=params.IMAGE_SIZE\n",
    "        )\n",
    "\n",
    "        return training_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        \"\"\"Initializes the Training class with a configuration object.\n",
    "        \n",
    "        Args:\n",
    "            config (TrainingConfig): Configuration object containing training parameters.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def load_base_model(self):\n",
    "        \"\"\"Loads the base model from the specified path.\"\"\"\n",
    "        if not self.config.updated_base_model_path.exists():\n",
    "            raise FileNotFoundError(f\"Base model not found at {self.config.updated_base_model_path}\")\n",
    "        \n",
    "        self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        \n",
    "    def train_valid_generator(self):\n",
    "        \"\"\"Creates data loaders for training and validation datasets.\"\"\"\n",
    "        \n",
    "        self.val_transform = T.Compose([\n",
    "            T.Resize(self.config.params_image_size[:2]),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        if self.config.params_is_augmentation:\n",
    "            self.train_transform = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.RandomRotation(degrees=(-10, 10)),\n",
    "                T.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "                T.Resize(self.config.params_image_size[:2]),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406],  # Same mean/std as ImageNet\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.train_transform = self.val_transform\n",
    "\n",
    "        self.train_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=os.path.join(self.config.training_data, \"train\"),\n",
    "            transform=self.train_transform\n",
    "        )\n",
    "\n",
    "        self.valid_dataset = torchvision.datasets.ImageFolder(\n",
    "            root=os.path.join(self.config.training_data, \"valid\"),\n",
    "            transform=self.val_transform\n",
    "        )\n",
    "\n",
    "        self.train_loader = data.DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        self.valid_loader = data.DataLoader(\n",
    "            dataset=self.valid_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains the model using the training dataset.\"\"\"\n",
    "\n",
    "        print(f\"Training on device: {self.device}\")\n",
    "        print(\"Loading the model....\")\n",
    "        self.load_base_model()\n",
    "        print(\"Model loaded successfully.\")\n",
    "        print(\"Loading Data....\")\n",
    "        self.train_valid_generator()\n",
    "        print(\"Data loaded successfully.Now training started...\")\n",
    "\n",
    "        criterion = self.get_loss_function()\n",
    "        optimizer = self.get_optimizer(self.model, self.config.params_learning_rate)\n",
    "        scheduler = self.get_scheduler(optimizer)\n",
    "        best_val_acc = 0.0\n",
    "        epochs = self.config.params_epochs \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            train_accuracy = 0.0\n",
    "            loop = tqdm(self.train_loader, total=len(self.train_loader))\n",
    "            for images, labels in loop:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                train_accuracy += (preds == labels).sum().item()\n",
    "                loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "                loop.set_postfix(loss=loss.item(), acc=train_accuracy)\n",
    "                break # just to test the code, remove this line to train on all data , i used this for testing my pipeline\n",
    "            epoch_loss = running_loss / len(self.train_dataset)\n",
    "            epoch_accuracy = train_accuracy / len(self.train_dataset) * 100\n",
    "            epoch_val_accuracy,epoch_val_loss = self.get_validation_metrics(self.model,\n",
    "                                                                            self.valid_loader,\n",
    "                                                                            self.device,\n",
    "                                                                            len(self.valid_dataset))\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}],\\\n",
    "                    Loss: {epoch_loss:.4f}, \\\n",
    "                    Train Accuracy: {epoch_accuracy:.3f},\\\n",
    "                    Validation Loss: {epoch_val_loss:.4f},\\\n",
    "                    Validation Accuracy: {epoch_val_accuracy:.3f}%\")\n",
    "\n",
    "            if epochs > 2: \n",
    "                scheduler.step(epoch_val_loss)\n",
    "            if epoch_val_accuracy > best_val_acc:\n",
    "                self.save_model(self.model,  self.config.trained_model_path.parent / f\"best_model_{epoch_val_accuracy:.3f}.pth\" )#os.path.join(self.config.trained_model_path.parent, \"best_model.pth\" ) )\n",
    "                best_val_acc= epoch_val_accuracy\n",
    "\n",
    "        self.save_model(self.model, self.config.trained_model_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_loss_function():\n",
    "        \"\"\"Returns the loss function for training.\"\"\"\n",
    "        return nn.CrossEntropyLoss()\n",
    "    \n",
    "    @staticmethod   \n",
    "    def get_optimizer(model, learning_rate):\n",
    "        \"\"\"Returns the optimizer for training.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to optimize.\n",
    "            learning_rate (float): Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        print(type(model))\n",
    "        return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_scheduler(optimizer):\n",
    "        \"\"\"Returns the learning rate scheduler.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: The optimizer to schedule.\n",
    "        \"\"\"\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5, verbose=True )    \n",
    "\n",
    "    @staticmethod\n",
    "    def get_validation_metrics(model, valid_loader, device,len_val_dataset):\n",
    "        \"\"\"Calculates the validation accuracy of the model.\n",
    "        \n",
    "        Args:\n",
    "            model: The trained model.\n",
    "            valid_loader: DataLoader for the validation dataset.\n",
    "            device: Device to run the model on (CPU or GPU).\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        accuracy = 0.0\n",
    "        loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                criterion = Training.get_loss_function()\n",
    "                loss += criterion(outputs, labels).item() * images.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                accuracy += (preds == labels).sum().item()\n",
    "                break # just to test the code, remove this line to validate on all data , i used this for testing my pipeline\n",
    "\n",
    "        val_accuracy = accuracy / len_val_dataset * 100\n",
    "        val_loss = loss / len_val_dataset\n",
    "\n",
    "        return val_accuracy, val_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(model, path):\n",
    "        \"\"\"Saves the trained model to the specified path.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to save.\n",
    "            path (Path): Path to save the model.\n",
    "        \"\"\"\n",
    "        torch.save(model, path)\n",
    "        print(f\"Model saved at {path}\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(model_path,device):\n",
    "        '''\n",
    "        Loads a updated VGG16 model with customized classifier from the specified path.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to the model file.'''\n",
    "        model = torch.load(model_path, map_location=device)\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a789fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-13 14:56:40,825|(INFO)| File: common | Message: Created directory: artifacts]\n",
      "[2025-07-13 14:56:40,831|(INFO)| File: common | Message: Created directory: artifacts/train_model]\n",
      "Training on device: cpu\n",
      "Loading the model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10534/548824563.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model = torch.load(self.config.updated_base_model_path, map_location=self.device)\n",
      "/home/kirti/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "/home/kirti/anaconda3/envs/deeplearning/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Loading Data....\n",
      "Data loaded successfully.Now training started...\n",
      "<class 'torchvision.models.vgg.VGG'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1]:   0%|          | 0/77 [00:32<?, ?it/s, acc=2, loss=1.44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1],                    Loss: 0.0188,                     Train Accuracy: 0.326,                    Validation Loss: 0.0655,                    Validation Accuracy: 11.111%\n",
      "Model saved at artifacts/train_model/best_model_11.11111111111111.pth\n",
      "Model saved at artifacts/train_model/model.pth\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigManager()\n",
    "    training_config = config.get_training_config()\n",
    "    trainer = Training(training_config)\n",
    "    trainer.train()\n",
    "except Exception as e:  \n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c26f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/kirti/anaconda3/envs/deeplearning/lib/python3.11/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929daa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
